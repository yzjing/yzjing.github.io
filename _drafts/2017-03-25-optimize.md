---
layout: post
title: Tracking time and memory usage in Python
categories: [English post]
<!---->tags: [coding, optimization]
comments: true

---

To optimize code, the first thing is to figure out which part of the code caused the slowliness/bulkiness. That is, we need a profile of the time/memory consumption for each part, ideally each line, of the code. 

For time profiling, Python comes with very good libraries [cProfile and Profile](https://docs.python.org/3.5/library/profile.html). cProfile is faster, so I went with it. 

It is easy to use the library:

`python -m cProfile -s cumtime unigram_turing_opt.py`

This is a screenshot of the output:

![image]({{site.baseurl}}/img/2017-03-25-time.png)

There are a few columns; `s` specifies which column to sort by. Here I sort by `cumtime`, so the code taking the most cumulative time is shown at the top. It is also easier to redirect the output to a file and `head` the file.

cProfile can also be used to time a single function. There is no need for me to describe the usage here.

We can also use external libraries. Robert Kern has a [line_profiler](https://github.com/rkern/line_profiler) that allows line-to-line profiling. cProfile seems enough for me so I didn't really use it. It's well documented, so no more notes is needed here too.

For memory profiling, I used a library [memory_profler](https://pypi.python.org/pypi/memory_profiler) that is similar to the line_profiler. It can also profile the entire script or just one function. To profile my main() function, I can put a decorator before it:

	from memory_profiler import profile

	@profile
	def main(fandom):
    	print('working on fandom: ', fandom)
    	df = pd.read_csv(fandom + '_preprocessed.tsv', sep = '\t')

The output:

![image]({{site.baseurl}}/img/2017-03-25-memory.png)

I also read a nice blog post [How to profile memory usage in Python](https://www.pluralsight.com/blog/tutorials/how-to-profile-memory-usage-in-python) that explains how to use the library in details.

