<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Elise Jing</title>
    <description>Personal site &amp; blog</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 16 May 2017 19:40:27 +0000</pubDate>
    <lastBuildDate>Tue, 16 May 2017 19:40:27 +0000</lastBuildDate>
    <generator>Jekyll v3.4.0</generator>
    
      <item>
        <title>Artificial Intelligence and Human Mind</title>
        <description>&lt;p&gt;Should we think of the brain as a computer? Many cognitive scientists will say yes: the computer metaphor has so deeply rooted in people’s understanding about brains, that it has become a default way of thinking. We’re familiar with the saying “recall my memory”, “process the information” and more, without thinking much about them. It can be hard to image this metaphor only has a history of less than a century. Before computers came into being, in ancient, medieval and modern times, the brain was compared to catapults, mills, and telegraphs: all the most advanced machines at certain historical periods (Searle, 1984).&lt;/p&gt;

&lt;p&gt;So the computer metaphor is not the only metaphor: it’s just the metaphor of our age. It has indeed been the dominating paradigm for several decades, only recently started to be questioned seriously by people. Theories that challenges it include the embodied mind hypothesis, the dynamical system theory, etc. However, it is not clear if any of them can take over the place.&lt;/p&gt;

&lt;p&gt;The other side of the coin is an opposite question: should we make computers by simulating the human mind? This has been a long-lasting debate in the philosophy of artificial intelligence. Nowadays, most researchers in AI are not very interested in how the mind works, and the mainstream approaches to AI do not intentionally simulate humans’ cognitive processes; similarities, if exist, are on an abstract level. However, there are still some heretics that insist on building “real” AI by making it work like the human minds do.&lt;/p&gt;

&lt;p&gt;Neither cogsci or AI is my main field, and I can’t say I’m very knowledgable on their theories. But I’m interested in the history and interplay between these thoughts, because I always like to learn how an idea comes into being as much as the idea itself. So when I took a &lt;a href=&quot;http://pages.iu.edu/~colallen/Courses/Q540/&quot;&gt;very cool course with Professor Colin Allen&lt;/a&gt;, I wrote a review paper on this topic. This post is a summary about some interesting questions that I read and wrote about. Most of it comes from the paper, but I rearranged the structure and “toned down” it a little to make it more suitable for a blog post.&lt;/p&gt;

&lt;p&gt;Additionally, I’m not attempting to propose an answer for any of those questions. I probably have indicated my position in some places, but they are only personal preference, rather than serious arguments.&lt;/p&gt;

&lt;h3 id=&quot;how-does-the-theory-of-computation-influence-the-theory-of-mind&quot;&gt;How does the theory of computation influence the theory of mind?&lt;/h3&gt;

&lt;p&gt;In a word, the influence is fundamental, especially at the beginning. The theory of computation started in 1930s by pioneers such as Alan Turing and Alonzo Church. The most iconic event was the invention of the Turing machine, an abstract machine that can carry out general computation by manipulating symbols. Soon after, psychologists started to think about whether the Turing machine can work as a model of the human mind. This idea developed into the computational theory of mind, which played a central role when cognitive science rose as an interdisciplinary area, making itself the “default” paradigm for the following fifty years.&lt;/p&gt;

&lt;p&gt;The computational paradigm has several important school of thoughts, depending on what “computation” is taken to be. In some senses, it was only used as a high-level metaphor for how the brain functioned, while in others, the brain was thought to be literally carrying out computation.&lt;/p&gt;

&lt;p&gt;The latter approach was accepted by Hilary Putnam (Putnam 1961) and introduced into the philosophy of mind, constructing the widely accepted theory: machine functionalism. In this view, mental states are functional states; the brain moves between functional states similar to how a Turing machine moves between machine states.&lt;/p&gt;

&lt;p&gt;About one decade later, Jerry Fodor introduced another main thread of the computational theory, the representational theory of mind (Fodor 1975, 1981). According to him, the brain as a Turing-style machine operates on mental symbols, which represents both basic and complex concepts about the world.&lt;/p&gt;

&lt;h3 id=&quot;how-does-psychology-and-neuroscience-influence-ai-&quot;&gt;How does psychology and neuroscience influence AI ?&lt;/h3&gt;

&lt;p&gt;There may have been some influence, but much less than the opposite case, and exist on a more abstract level. A few direct influences can be noted, mainly with the artificial neural network models: when creating the Perceptron, Frank Rosenbratt, a psychologist, was inspired by real neurons (Rosenblatt, 1957). The closely associated Back Propagation algorithm, without which artificial neural networks would not work well, was also loosely inspired by Freud’s psychological theories (Werbos 1974).&lt;/p&gt;

&lt;h3 id=&quot;are-there-more-interactions-between-the-two-fields&quot;&gt;Are there more interactions between the two fields?&lt;/h3&gt;

&lt;p&gt;It is interesting that the two fields seem to have experienced paradigm shifts at similar times. As the computational theory of mind becomes “the only game in town”, the symbolic approach to AI was also at the best of its days, before going into the first AI winter in mid-1970s. Leading researchers at CMU, MIT, and other institutes believed that symbolic is the ultimate approach to AI, claiming that it will completely solve the problem of creating AI in 20 years (Minsky 1967).&lt;/p&gt;

&lt;p&gt;They even go further to argue that symbol manipulation is at the root of all intelligence, and when combined with proper physical components, “any physical symbol system of sufficient size can be organized further to exhibit general intelligence” (Newell and Simon, 1976). As we know, the task was soon proved to be much more difficult than they optimistically thought, and by 1980s symbolic AI was no longer the prevailing paradigm.&lt;/p&gt;

&lt;p&gt;It may also be not a coincidence that connectionism, challenger of the classical computational theory in influence, also rose at this point. In this case, however, the psychologists didn’t directly borrow ideas from computational theorists. Rather, the paradigm found its way into both disciplines, fueled by neuroscience and computer science together (Medler, 1998). Connectionism is a major diverging from the symbolic approach: connected units (more specifically, neurons) operate under certain rules, and what were traditionally thought to be stored as symbols — memory, concepts, propositions — are claimed to be contained in these connections.&lt;/p&gt;

&lt;p&gt;Generally speaking, the two fields shared important ideas and experienced significant changes at approximately the same time. Additionally, many great scientists in one area also had substantial contributions to the other, for example, Alan Turing and Herbert Simon. These all imply that they are deeply intertwined in some ways.&lt;/p&gt;

&lt;h3 id=&quot;are-human-intelligence-and-artificial-intelligence-fundamentally-the-same&quot;&gt;Are human intelligence and artificial intelligence fundamentally the same?&lt;/h3&gt;

&lt;p&gt;To consider this question, we have to start with a definition for intelligence.  A common definition is that an intelligent being should be able to perceive and react to the environment, which is what human and many animals are capable of; but according to this, plants such as sun flowers will also have intelligence. Stricter definitions involve the ability to think and reason, but when it comes to machines, these abilities are hard to identify. Can we say that a machine learning program is thinking and reasoning when it makes classifications?&lt;/p&gt;

&lt;p&gt;This is exactly the question Turing asked more than 60 years ago (Turing 1950), and he got around it by proposing a “polite convention”: if a machine acts as intelligently as a human, then it is as intelligent as a human. The well-known Turing test is based on this premise. However, although it has been popular as both a thought experiment and a real experiment, we should be aware that the original question had been modified in a behaviorist-style way. A machine acting as if it is thinking does not necessarily imply that it is thinking, as indicated by the problem of other minds.&lt;/p&gt;

&lt;p&gt;As Turing pointed out, to face the question directly requires us to go into the definition of thinking. If as the classical computational theory of mind suggests, thinking is the manipulation of symbols, then it seems reasonable to consider a computer to be thinking. The latter may be what the symbolic AI supporters have in mind: when Newell and Simon described their model of physical symbol systems, they did not limit it to either computers or human, because “the symbolic behavior of man arises because he has the characteristics of a physical symbol system” (Newell and Simon, 1975), and computer and human thinking are, in their roots, the same thing. If instead as connectionists say, thinking is the result of activities of connected neurons, then it’s likely not going to be found in the computers we currently have.&lt;/p&gt;

&lt;p&gt;This suggests us to turn once more to the long-lasting debate between classical computational theory of mind and connectionism. However, many have also suggested that the two paradigms are not mutually exclusive. A neural network can be implemented in a computer that does symbolic manipulation; oppositely, Turing-style computation can also be achieved using a neural network (Rescorla, 2015).&lt;/p&gt;

&lt;p&gt;Regarding physical implementations, although currently we don’t have computers made of silicon neurons, in the future they may come into being. Reversely, it has been attempted to simulate the whole human brain in a supercomputer, mapping over a hundred billion neurons and their connections. Although the project has not been successful (Dvorsky 2014), it implies the possibility of completely “transplanting” a brain to a computer. In that case, is the simulated brain or the computer thinking? If the boundary between brains and machines gets obscure, perhaps the title of this section will become an unnecessary question.&lt;/p&gt;

&lt;h3 id=&quot;is-it-necessary-to-simulate-the-human-minds-to-create-ai&quot;&gt;Is it necessary to simulate the human minds to create AI?&lt;/h3&gt;

&lt;p&gt;This question partly depends on a premise — do we know how the human mind works in the first place? Depending on the answer to these two questions, various positions may be taken. Some symbolic AI supporters would say “yes” to both: for example, Newell and Simon intentionally mimicked human’s problem solving behavior, found through psychological experiments (Daniel 1993), to create their algorithms. Some others did not think it was necessary to simulate human behavior, and created algorithms for AI that might or might not behave as some humans would have. Most statistical approaches to AI are thought to be different from human thinking. However, artificial neural network models are often considered an analogy to human’s learning process.&lt;/p&gt;

&lt;p&gt;It may be worthwhile to go deeper into this example. A type of neural networks excelling at computer vision tasks is the convolutional neural network model. Intuitively, a convolution of two matrices is a way of “blending” them. In convolutional neural networks, it is applied to draw a representation from every local region of the image, therefore reducing the number of parameters to train. When the features are passed down hidden layers, this process is repeated. The primary purpose of applying this algorithm is to optimize the model. However, when used in computer vision tasks, it appears to work similarly to how the human visual cortex processes images: after sufficient training, a layer will often be found to specialize in identifying the outline of objects, a second layer will specialize in processing colors, etc. In a 2014 paper, Cox and Dean suggested a rough mapping between layers in a convolutional neural network and regions in a visual cortex (Cox and Dean, 2014).&lt;/p&gt;

&lt;p&gt;In this example, artificial intelligence works like the human brain to a certain degree (although major differences still exist, for example, the brain doesn’t have a back propagation mechanism). However, there was no intentional simulation when the model was created. Inspiration from the brain, although may have existed, is superficial. Similarly, most neural network models are not directly simulating human’s cognitive processes.&lt;/p&gt;

&lt;p&gt;But these models perform well, being able to recognize cats in pictures (Le, 2013), find semantic similarities, and play Go as well as humans do. With such success, it seems natural that most AI researchers nowadays do not care much about how human mind works. However, there are still people who insist that the only way to create “full” AI is by replicating human mind in a machine.&lt;/p&gt;

&lt;h3 id=&quot;will-ais-have-minds&quot;&gt;Will AIs have minds?&lt;/h3&gt;

&lt;p&gt;Despite the recent success of deep learning with neural network models, some people, for example Douglas Hofstadter, still insists that this is fundamentally wrong, because a machine like IBM’s Watson “is finding text without having a clue as to what the text means. In that sense, there’s no intelligence there.” (Herkewitz, 2014).&lt;/p&gt;

&lt;p&gt;This easily reminds us of Searle’s Chinese room case (If you’d like to read about his argument, see &lt;a href=&quot;https://plato.stanford.edu/entries/chinese-room/&quot;&gt;here&lt;/a&gt;. The thought experiment was used to refute the strong AI hypothesis, that a properly programmed computer handling inputs and outputs as a human does has the same mind as the human does. In Searle’s argument, a computer that matches Chinese output with English input according to a set of rules cannot be said to understand Chinese (Searle 1980). He further concluded that no physical symbol system could ever be said to have a mind. There has not been a full consensus on this argument, but many consider it to be sound.&lt;/p&gt;

&lt;p&gt;Searle’s attack being directed to the symbolic AI approach, we may wonder if it’s the case with neural network models, which are famous for being “black boxes” that don’t let people understand what is happening between their layers. Will a complicated neural network have a mind? An interesting reply to Searle’s argument is the emergence hypothesis. In a complex system, the properties of a system as a whole are not the same as adding up the properties of the parts (Simon, 1962). Some suggest that if an artificial intelligence is complex enough, a mind may emerge spontaneously by interaction between its parts. This is a popular imagination in contemporary culture: AIs in many cyberpunk fictions gain consciousness in this way. But in this way, the hard problem of consciousness is still not solved. Just as with ourselves, we observe the existence of consciousness, but don’t know how it rises from a lot of neurons (or a lot of circuits).&lt;/p&gt;

&lt;p&gt;Furthermore, the relation between mind and general intelligence is also unsolved. Considered to be the “ultimate goal” of AI, general intelligence is usually defined as the ability of performing a wide range of intelligent actions: learning, reasoning, working towards a goal, and more — in a word, things that a normal human adult can do. Naturally, the question is often asked: does a being must have a mind to have general intelligence, or is mind not relevant to having general intelligence at all?&lt;/p&gt;

&lt;p&gt;If we take the first option, the current mainstream approach to AI will be fundamentally wrong. However, in the brief historical review above, it can be observed that unless development of technology becomes stilled, there will be little motivation for researchers to push for paradigm shifts. When symbolic AI and expert systems are making significant advances, few people considered them to be wrong approaches (although we should acknowledge that at that time, the computational power was also not sufficient to support other approaches such as neural networks). With the current success of deep learning models, it may be reasonable to infer a similar situation: unless at a certain point the advance of such models hit a dead end, it will be unlikely for people to switch to a different approach.&lt;/p&gt;

&lt;p&gt;The second option, naturally, is favored by most current researchers. Why is it necessary to have a mind in order to display general intelligence? It seems not hard to put a Chinese room, a German room, a Russian room,… together to create a general machine translator that doesn’t understand any of these languages, which is essentially Google Translate. Similarly, it may be possible that by putting the specific artificial intelligences together, the AI built will display general intelligence without anything like mind or intentionality.&lt;/p&gt;

&lt;p&gt;This invites us to go back to Turing’s hypothesis from a slightly different view. If an AI displays general intelligence as a human does, how do we know if it has a mind or not? In the famous thought experiment of philosophical zombies, it is claimed impossible to tell if anyone other than oneself has a mind, or is a complicated automata that behaves just like it has a mind: take the example of pain, it can be programmed to wince when approaching fire as if it feels pain, while it actually has no feelings of pain. If we are unable to tell whether another person has a mind, are we able to judge whether an AI has a mind?&lt;/p&gt;

&lt;p&gt;If we take this claim, the boundary between humans and computers is again obscured: a human can now be considered as a computer that successfully passed the Turing test. Moreover, being able to behave like a human will have nothing to do with having a mind. It seems yet another support for the claim that having a mind is not relevant to displaying general intelligence.&lt;/p&gt;

&lt;p&gt;However, most people are likely not comfortable to be skeptical enough to accept a world walked by zombies. If in the future a computer exhibiting perfect general intelligence is created, we may be not able to tell if it has a mind at all. But this will surely jeopardize the belief held by many, as expressed in the movie &lt;em&gt;Ghost in the Shell&lt;/em&gt;: that what distinguishes a human from a machine is the awareness of a self – a “ghost”, a mind, a soul.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;Cox, D. D., &amp;amp; Dean, T. (2014). Neural networks and neuroscience-inspired computer vision. Current Biology, 24(18), R921-R929.&lt;/p&gt;

&lt;p&gt;“Dartmouth workshop”. (2017, April 23). Retrieved from https://en.wikipedia.org/wiki/Dartmouth_workshop&lt;/p&gt;

&lt;p&gt;Dvorsky, G. (2014, July 10). “Europe’s $1.6 Billion Human Brain Project Is On The Verge Of Collapse”. Retrieved from http://io9.gizmodo.com/europes-1-6-billion-human-brain-project-is-on-the-verg-1602991993&lt;/p&gt;

&lt;p&gt;Epstein, R. (2016, May). “The empty brain”. Retrieved from https://aeon.co/essays/your-brain-does-not-process-information-and-it-is-not-a-computer&lt;/p&gt;

&lt;p&gt;Fodor, J., 1975, The Language of Thought, New York: Thomas Y. Crowell.&lt;/p&gt;

&lt;p&gt;Fodor, J., 1981, Representations, Cambridge: MIT Press.&lt;/p&gt;

&lt;p&gt;Herkewitz, W. (2014, February). “Why Watson and Siri Are Not Real AI”. Retrieved from http://www.popularmechanics.com/science/a3278/why-watson-and-siri-are-not-real-ai-16477207/&lt;/p&gt;

&lt;p&gt;Le, Q. V. (2013, May). Building high-level features using large scale unsupervised learning. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on (pp. 8595-8598). IEEE.&lt;/p&gt;

&lt;p&gt;McCulloch, W. and W. Pitts, 1943, “A Logical Calculus of the Ideas Immanent in Nervous Activity”, Bulletin of Mathematical Biophysics, 7: 115–133.&lt;/p&gt;

&lt;p&gt;Medler, D. A. (1998). A brief history of connectionism. Neural Computing Surveys, 1, 18-72.&lt;/p&gt;

&lt;p&gt;Minsky, Marvin (1967). Computation: Finite and Infinite Machines. Englewood Cliffs, N.J.: Prentice-Hall. ISBN 0-13-165449-7. Quoted in Crevier, Daniel (1993), AI: The Tumultuous Search for Artificial Intelligence, New York, NY: BasicBooks, ISBN 0-465-02997-3.&lt;/p&gt;

&lt;p&gt;Newell, A., &amp;amp; Simon, H. A. (1976). Computer Science as Empirical Enquiry: Symbols and Search, 19 COMM. ASS’N FOR COMPUTING MACHINERY, 113.&lt;/p&gt;

&lt;p&gt;Putnam, Hilary. 1961. “Brains and Behavior”, originally read as part of the program of the American Association for the Advancement of Science, Section L (History and Philosophy of Science), December 27, 1961. Reprinted in Block (1980).&lt;/p&gt;

&lt;p&gt;Rescorla, M. (2015, Oct 16). “The Computational Theory of Mind”. Retrieved from https://plato.stanford.edu/entries/computational-mind/&lt;/p&gt;

&lt;p&gt;Rosenblatt, F. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957.&lt;/p&gt;

&lt;p&gt;Searle, J. R. (1980). Minds, brains, and programs. Behavioral and brain sciences, 3(03), 417-424.&lt;/p&gt;

&lt;p&gt;Searle, J. R. (1984). Minds, brains and science. Harvard University Press.&lt;/p&gt;

&lt;p&gt;Simon, H. A. (1962). The architecture of complexity. Proceedings of the American philosophical society, 106(6), 467-482.&lt;/p&gt;

&lt;p&gt;Simon, H. A. (1965). The Shape of Automation for Men and Management. New York: Harper &amp;amp; Row.&lt;/p&gt;

&lt;p&gt;Turing, A. M. (1950). Computing machinery and intelligence. Mind, 59(236), 433-460.&lt;/p&gt;

&lt;p&gt;Cervier, D. (1993). AI: The Tumultuous Search for Artificial Intelligence.&lt;/p&gt;

&lt;p&gt;Werbos, P. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University, Cambridge, MA.&lt;/p&gt;

</description>
        <pubDate>Mon, 15 May 2017 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-05/ai-mind</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-05/ai-mind</guid>
        
        
        <category>English post</category>
        
      </item>
    
      <item>
        <title>Notes on Time &amp; Memory optimization with Python</title>
        <description>&lt;p&gt;In my Fanfic project I had to work with text data that adds up to ~36G (in CSV format). The server I use has 200G RAM, which seems big, but is actually not much when you manipulate data with this size. So, after repeatingly waiting for 3 days and getting a memory error, I realized I have to work on optimizing my code, which I had little training on. Therefore I’ll put my notes on how to do it here, so I don’t have to Google it next time.&lt;/p&gt;

&lt;h2 id=&quot;finding-the-bottle-necks&quot;&gt;Finding the bottle necks&lt;/h2&gt;
&lt;p&gt;Instead of randomly shooting, a smarter way is to first figure out which part of the code caused the slowliness/bulkiness. That is, we need a profile of the time/memory consumption for each part, ideally each line, of the code.&lt;/p&gt;

&lt;p&gt;For time profiling, Python comes with very good libraries &lt;a href=&quot;https://docs.python.org/3.5/library/profile.html&quot;&gt;cProfile and Profile&lt;/a&gt;. cProfile is faster, so I went with it.&lt;/p&gt;

&lt;p&gt;It is easy to use the library:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;python -m cProfile -s cumtime unigram_turing_opt.py&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is a screenshot of the output:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-03-25-time.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are a few columns; &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt; specifies which column to sort by. Here I sort by &lt;code class=&quot;highlighter-rouge&quot;&gt;cumtime&lt;/code&gt;, so the code taking the most cumulative time is shown at the top. It is also easier to redirect the output to a file and &lt;code class=&quot;highlighter-rouge&quot;&gt;head&lt;/code&gt; the file.&lt;/p&gt;

&lt;p&gt;cProfile can also be used to time a single function. There is no need for me to describe the usage here.&lt;/p&gt;

&lt;p&gt;We can also use external libraries. Robert Kern has a &lt;a href=&quot;https://github.com/rkern/line_profiler&quot;&gt;line_profiler&lt;/a&gt; that allows line-to-line profiling. cProfile seems enough for me so I didn’t really use it. It’s well documented, so no more notes is needed here too.&lt;/p&gt;

&lt;p&gt;For memory profiling, I used a library &lt;a href=&quot;https://pypi.python.org/pypi/memory_profiler&quot;&gt;memory_profler&lt;/a&gt; that is similar to the line_profiler. It can also profile the entire script or just one function. To profile my main() function, I can put a decorator before it:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from memory_profiler import profile

@profile
def main(fandom):
	print('working on fandom: ', fandom)
	df = pd.read_csv(fandom + '_preprocessed.tsv', sep = '\t')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The output:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-03-25-memory.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also read a nice blog post &lt;a href=&quot;https://www.pluralsight.com/blog/tutorials/how-to-profile-memory-usage-in-python&quot;&gt;How to profile memory usage in Python&lt;/a&gt; that explains how to use the library in details.&lt;/p&gt;

&lt;h2 id=&quot;optimizing-notes&quot;&gt;Optimizing notes&lt;/h2&gt;
&lt;p&gt;By doing the profiling described above and analyzing the reports, I was able to shorten the runtime of my code from 3 days to ~1.5 hours. Here are some main take aways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Don’t use NLTK. Just don’t.&lt;/li&gt;
  &lt;li&gt;When working with Pandas dataframes, take out the unused columns ASAP.&lt;/li&gt;
  &lt;li&gt;Pandas’s &lt;code class=&quot;highlighter-rouge&quot;&gt;apply&lt;/code&gt; can be slow if you do it row by row (&lt;code class=&quot;highlighter-rouge&quot;&gt;axis=1&lt;/code&gt;). &lt;code class=&quot;highlighter-rouge&quot;&gt;map&lt;/code&gt; can be better, because &lt;code class=&quot;highlighter-rouge&quot;&gt;map&lt;/code&gt; operates on the series and &lt;code class=&quot;highlighter-rouge&quot;&gt;apply&lt;/code&gt; on the dataframe.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 25 Mar 2017 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-03/optimize</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-03/optimize</guid>
        
        
        <category>English post</category>
        
      </item>
    
      <item>
        <title>以有涯而随无涯</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;
迁博第一篇，想谈论一个比较大的问题，也算是回顾一下读博读到一半（希望是一半……）这段时间里学过和考虑过的一些东西。
&lt;br /&gt;
&lt;br /&gt;
大概三年前我在知乎看到过一个问题：“要更好地理解世界，需要建立怎样的知识体系？”。我写了一个答案，接着又觉得没什么意思，就没有发出去。最近翻到那个文档看了一遍，意外地发现当时自己的不少观点现在看来仍然能够认同，尤其是关于何为“理解”这个认识论的问题。引用其中一句：
&lt;br /&gt;
&lt;br /&gt;

&lt;blockquote&gt;
&lt;p&gt;
规律可能只存在于理解中，而非世界本身；理解是将混沌无序的世界纳入符合人类认知方式的结构的过程。
&lt;/p&gt;
&lt;/blockquote&gt;

一直以来无论在学术领域之内还是之外，我追求的都是“理解世界”这个目标，但我对于“理解”的看法始终是这样带有建构主义色彩的。George Klir 在一本关于系统科学的书里表达过有些激进的观点：我们通常所说的系统，比如蚁群、气候系统或者社会，并不独立于人类心智地存在于真实世界，而是有赖于一个观测者将它们识别出来并加以界定[1]。 我在很大程度上认同这样的立场：在我看来一切科学研究的成果、世间万物内在的规律，同样也是我们心智建构的产物。客观的、不变的“解释”并不存在，存在的只是因每个人的立场不同而千差万别的“诠释”。
&lt;br /&gt;
&lt;br /&gt;
采取这种立场与我个人的学科背景和接触过的思想传统有关，而其中最重要的大概是对外部世界的怀疑主义立场：简单来说，我无法确定我关于外部世界的知识是否正确。其实这基本上是一个 philosophical belief，思考可以改变有关它的论述细节甚至是一些次要观点，但无法改变 belief（我在哲学系修过一门关于这个问题的课，到课程结束的时候班里怀疑主义的支持者没有说服任何一个反对者改变立场，反对者也没有说服支持者）。既然我不知道自己对世界的认识是否可靠——更激进一点，甚至不确定是不是真的存在一个外部世界——那么脱离我自身来谈论一个“独立”的外部世界自然也是没有意义的。
&lt;br /&gt;
&lt;br /&gt;
另一个思想来源是我在大学里读的福柯，他关于知识与权力关系的论述对我影响很大：
&lt;br /&gt;
&lt;br /&gt;
&lt;blockquote&gt;
&lt;p&gt;
没有相应的知识领域的建立，就没有权力关系；如果不同时假定和建立权力关系，也没有知识可言。[2]
&lt;/p&gt;
&lt;/blockquote&gt;


作为一个人文学科出身而后转投理工科的人，科学是我偏好用以探究世界的方式，但我从不认为科学提供的是公正、客观的关于世界的事实。科学不是象牙塔里的求索，而是由生活在社会中的人类实践的一种 social practice，受到政治、资本的形塑，本身属于现代性这个庞大话语体系的一部分，而科学提供的“事实”同样也是这个话语体系的产物。
&lt;br /&gt;
&lt;br /&gt;
因此单独谈论“理解世界”这个话题并不能带给我们多少收获，这也是为什么我在这个网站首页介绍研究兴趣时是那样写的：更根本的问题是人如何理解世界。
&lt;br /&gt;
&lt;br /&gt;
而科学能够带给我们这个问题的答案吗？我不知道，我知道的只是对现代人来说，要在科学框架内获得回答这个问题所需的知识实在太难了，在人类积累的知识量极度膨胀的时代，对方方面面的知识都有所涉猎几乎是不可能的。文艺复兴时代达芬奇式的全才、理性主义时期笛卡尔、莱布尼茨式跨学科的通才早已不复再有，如今的研究者甚至连自己学科的各个分支也无法完全掌握。而这样一个问题又偏偏落在多个学科的交叉点上，以我认为最相关的领域之一——人工智能为例，要深入了解它就需要数学、计算机科学、控制论、可计算理论等方面的背景，而其中每一个都足以让人轻易耗去一生的时间。
&lt;br /&gt;
&lt;br /&gt;
如果把眼光投向科学之外又会怎样呢？理解世界的途径还有许多，宗教、艺术、音乐或是身体知觉。它们并不遵循科学的逻辑，有些甚至会回避我们根植于语言的思维方式，比如禅宗的公案，通过荒谬地违背语言中的逻辑令修行者跳出这一框架，达到一种不依托语言的理解或者说顿悟。
&lt;br /&gt;
&lt;br /&gt;
但这些途径有一个共同点，就是它们都是个人化的体验。这正好是科学的反面：不追求证据和可重复性。许多佛教徒声称体会到了自己和整个宇宙融为一体，超脱个体的局限而领悟了这个世界的真相。或许我以后也会去追求这种神秘主义的体验，但无论我经历了什么，它都无法被别人重复。
&lt;br /&gt;
&lt;br /&gt;
而如果这个目标最终归结为对个体经验的追求，未免也有些悲哀。毕竟我们一开始向往的总是一些普适的结果，一些不会在我们死后便被人遗忘、能够长久流传下去的东西。作为渺小而速朽的个体，这种对永恒的追求或许是一种对抗虚无的本能吧。
&lt;br /&gt;
&lt;br /&gt;
我不知道哪一种方式最终通向我追求的答案，事实上在悲观主义和失败主义的思绪上涌时，我倾向于认为这个答案远在我能触及的范围之外，或者根本不存在：宇宙并没有义务被人理解[3]。但这也是一种价值取向——我并不真的相信我为之努力的事情，也只是这让我能够继续为之付出努力。
&lt;br /&gt;
&lt;br /&gt;
天地苍茫，每个探索者终有穷途末路的时刻，在我们之前的思想者早已经历过，我们之后的也将会经历。中国的思想传统里一直有“知其不可而为之”这样的做法，有些事情总是想要去做，无论最后得到的是什么。毕竟——引用周云蓬的一句诗：
&lt;br /&gt;
&lt;br /&gt;
&lt;blockquote&gt;
&lt;p&gt;
能看见什么，不能看见什么，这是我们的宿命。
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;br /&gt;
2017.3.3 于 Bloomington
&lt;br /&gt;
&lt;br /&gt;
[1] George Klir, Facets of System Science
&lt;br /&gt;
[2]  Michel Foucault, Discipline and Punish: The Birth of the Prison
&lt;br /&gt;
[3] 引自天文学家 Caleb Scharf

&lt;/font&gt;
</description>
        <pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/articles/2017-03/%E4%BB%A5%E6%9C%89%E6%B6%AF%E8%80%8C%E9%9A%8F%E6%97%A0%E6%B6%AF</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2017-03/%E4%BB%A5%E6%9C%89%E6%B6%AF%E8%80%8C%E9%9A%8F%E6%97%A0%E6%B6%AF</guid>
        
        
        <category>中文博文</category>
        
      </item>
    
  </channel>
</rss>
